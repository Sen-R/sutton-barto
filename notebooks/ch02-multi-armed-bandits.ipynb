{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f87bed5d-b9e0-457c-8795-c8a59386e523",
   "metadata": {},
   "source": [
    "# Chapter 2: Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da5c27-2dd1-4f02-8c02-54d3098ebe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2c159-3f5a-4838-ad15-d60339f5cfb0",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "Q: In $\\varepsilon$-greedy action selection, for the case of two actions and $\\varepsilon = 0.5$, what is the probability that the greedy action is selected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e02bf-4d51-48a5-a9fc-8369a64655fc",
   "metadata": {},
   "source": [
    "**A**: In $\\varepsilon$-greedy action selection, the greedy action be selected in one of two ways:\n",
    "1. With probability $1 - \\varepsilon$ it is chosen as the exploitation action;\n",
    "2. With probability $\\tfrac{\\varepsilon}{|\\mathcal{A}|}$ (where $\\mathcal{A}$ is the action space), it could be chosen at random as an exploration action.\n",
    "\n",
    "These are mutually exclusive events, so the total probability of the greedy action being chosen is\n",
    "\n",
    "$$1 - \\varepsilon + \\frac{\\varepsilon}{|\\mathcal{A}|}.$$\n",
    "\n",
    "In the question, $|\\mathcal{A}| = 2$ and $\\varepsilon = 0.5$. Hence the greedy action is chosen with probability\n",
    "\n",
    "$$1 - 0.5 + \\frac{0.5}{2} = 0.75.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b16e5c-0eb1-4e21-b204-d708dd6c0f6a",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "\n",
    "Q: Consider a $k$-armed bandit problem with $k=4$ actions, denoted 1, 2, 3 and 4. Consider applying to this problem a bandit algorithm using $\\varepsilon$-greedy action selection, sample-avarage action-value estimates, and initial estimates of $Q_1(a) = 0$ for all $a$. Suppose the initial sequence of actions and rewards is\n",
    "\n",
    "$$i$$ | $$A_i$$ | $$R_i$$\n",
    ":-: | :-: | :-:\n",
    " 1 | 1 | 1 \n",
    " 2 | 2 | 1\n",
    " 3 | 2 | 2\n",
    " 4 | 2 | 2\n",
    " 5 | 3 | 0\n",
    "\n",
    "On some of these times steps the $\\varepsilon$ case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12e614-3185-4405-b15c-6097eff2656e",
   "metadata": {},
   "source": [
    "A: We can manually calculate the action values as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec9866b-d9f2-47ad-9b89-a98cccb93c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data\n",
    "action_space = [1, 2, 3, 4]\n",
    "actions = [1, 2, 2, 2, 3]\n",
    "rewards = [1, 1, 2, 2, 0]\n",
    "\n",
    "# Set up initial Q values and action counts N for i=1\n",
    "Q = {a: 0 for a in action_space}\n",
    "N = {a: 0 for a in action_space}\n",
    "\n",
    "# Iteratively update N and Q for remaining time steps and save Q history\n",
    "Q_history = []\n",
    "for i, (A, R) in enumerate(zip(actions, rewards), 1):\n",
    "    Q_history.append(Q.copy())\n",
    "    N[A] += 1\n",
    "    Q[A] = Q[A] + (1.0 / N[A]) * (R - Q[A])\n",
    "\n",
    "# Display table of action values\n",
    "(\n",
    "    pd.DataFrame.from_records(Q_history, index=range(1, len(actions) + 1))\n",
    "    .rename(lambda a:f\"$$Q({a})$$\", axis=1)\n",
    "    .rename_axis(\"$$i$$\", axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7aa1c3-a7b0-45fb-b44f-1c0b46e272f1",
   "metadata": {},
   "source": [
    "From these we can determine whether a greedy action was chosen at each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a8136-8683-49b4-9097-d8546eeef166",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_action_values = [Q[A] for Q, A in zip(Q_history, actions)]\n",
    "greedy_action_values = [max(Q.values()) for Q in Q_history]\n",
    "definitely_exploration = [c != g for c, g in zip(chosen_action_values, greedy_action_values)]\n",
    "\n",
    "# Display results as a table\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"$$\\max_{a \\in \\mathcal{A}} Q_i(a)$$\": greedy_action_values,\n",
    "        \"$$Q_i(A_i)$$\": chosen_action_values,\n",
    "        \"Definitely exploration?\": definitely_exploration,\n",
    "    },\n",
    "    index=pd.Index(range(1, len(actions) + 1), name=\"$$i$$\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eba803-8949-4ecc-a9e1-0f4b189b4bed",
   "metadata": {},
   "source": [
    "So we can see that at timesteps $i=2$ and $i=5$, the chosen action was **not** the greedy action and hence exploration (the \"$\\varepsilon$ case\") definitely happened. For all other time steps, we do not know: perhaps the greedy action was chosen \"on purpose\", i.e. as an exploitation action, or perhaps it was still chosen as an exploration action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e2af57-7688-4a19-a9f6-ed6572ba5443",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "\n",
    "Q: In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97be395-b830-41bc-906f-a676f3e9574e",
   "metadata": {},
   "source": [
    "A: Figure 2.2 from the textbook shows average learning curves (over 2000 randomly chosen bandits) for $\\varepsilon$-greedy agents with $\\varepsilon = 0$, $0.01$ and $0.1$. Let's begin by first reproducing this figure. To save computation time, we use a testbed of only 200 bandits, but extend the simulation length to 10,000 steps to get a better view on long run behaviour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b52cf2-3af7-416e-8294-a7f8859744ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bandits import bandit_experiment, BanditResults\n",
    "from rl.environments.bandit import random_bandit\n",
    "from rl.agents import EpsilonGreedyRewardAveragingAgent\n",
    "\n",
    "# Set up experiment parameters\n",
    "entropy = 238402285\n",
    "n_levers = 10\n",
    "agents_to_test = {\n",
    "    fr\"$\\varepsilon = {epsilon}$\": partial(EpsilonGreedyRewardAveragingAgent, epsilon, n_levers)\n",
    "    for epsilon in [0.0, 0.01, 0.1]\n",
    "}\n",
    "bandit_builder = partial(\n",
    "    random_bandit, n_levers, mean_params=(0.0, 1.0), sigma_params=(1.0, 0.0)\n",
    ")\n",
    "n_steps = 10000\n",
    "test_bed_size = 200\n",
    "\n",
    "# Load saved results, or if not found, run experiment. Then plot results.\n",
    "results_file = f\"results/epsilon-bandit-results-n_steps-{n_steps}-test_bed_size-{test_bed_size}.pkl\"\n",
    "try:\n",
    "    results = BanditResults.load(results_file)\n",
    "except FileNotFoundError:\n",
    "    results = bandit_experiment(\n",
    "        agents_to_test, bandit_builder, test_bed_size, n_steps, logging_period=10, entropy=entropy, n_jobs=-1, verbose=1\n",
    "    )\n",
    "    results.save(results_file)\n",
    "results.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de8d90-1f1d-49ba-9e8c-34ddc9829e13",
   "metadata": {},
   "source": [
    "The first two of these plots are analogous to the 2 charts in Figure 2.2 of the textbook (but with a testbed of 200 bandits instead of 2,000 and with simulations running for 10,000 steps instead of 1,000). The last plot shows how often the greedy action as estimated by the agent (i.e. the action that maximises estimated $Q$) corresponds to the actual optimal action. In all the plots, we have also added for context a horizontal pink line corresponding to the performance of an optimal agent that chooses the optimal action at every step.\n",
    "\n",
    "We now return to answering the question.\n",
    "\n",
    "Let's start with the case where $\\varepsilon > 0$. In this case, as the simulation length goes to infinity, *all* states will be explored an infinite number of times and hence the agent's action value estimates for all states will converge to the true values. We can see this reflected in the righthand plot: the $\\varepsilon = 0.1$ agent has typically identified the optimal action by around 6,000 steps; the $\\varepsilon = 0.01$ agent will take much longer (as it will explore a given non-greedy action only approximately once every hundred steps) but will eventually get there.\n",
    "\n",
    "However, although the higher $\\varepsilon$ the quicker the optimal action is identified, it is also the cases that the higher $\\varepsilon$ the more frequently the agent will fail to choose the optimal action (at least, if we keep $\\varepsilon$ constant). This means that, in the long run, the average reward per step for an $\\varepsilon$-greedy agent with non-zero $\\varepsilon$ will ultimately converge to\n",
    "\n",
    "$$(1 - \\varepsilon)  R_\\mathrm{opt} + \\varepsilon R_\\mathrm{random}$$\n",
    "\n",
    "where $R_\\mathrm{opt}$ is the expected reward-per-step for an agent that always selects the optimal action and $R_\\mathrm{random}$ is the expected reward-per-step for an agent that always selects a random action. When we average over a large number of random bandits (whose mean rewards are drawn from a standard normal distribution and whose standard deviations are one, as specified in the book), then $R_\\mathrm{random} = 0$ and\n",
    "\n",
    "$$R_\\mathrm{opt} = \\mathbb{E}\\left[\\max(Z_1, Z_2, \\ldots, Z_{10})\\right]$$\n",
    "\n",
    "where $Z_1, \\ldots, Z_{10}$ are independent standard normal random variables. The pdf for the distribution for the max of these variables can be derived and the expectation numerically estimated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a9a70-814b-4bc6-b439-bdea28dd7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.integrate import quadrature\n",
    "\n",
    "# Since P(max(Z1, ..., Z10) < z) = P(Z1 < z & Z2 < z & ... & Z10 < z) = P(Z < z) ** 10\n",
    "pdf = lambda z: 10. * (norm.cdf(z) ** 9) * norm.pdf(z)  # derivative of cdf\n",
    "integrand = lambda z: z * pdf(z)\n",
    "print(\"Expected reward for perfect agent: {:.3f} (error: {:.1e})\".format(*quadrature(integrand, -10, 10, maxiter=1000)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ffc43-d5d3-4f07-8bf4-be86283fd78c",
   "metadata": {},
   "source": [
    "Indeed, as shown by the dashed horizontal line in the first plot, the expected reward for the testbed used for the experiment is indeed around 1.5.\n",
    "\n",
    "On the other hand, if $\\varepsilon = 0$, i.e. the bandit is fully greedy, then it will in the long run settle on one action (which may not be the optimal) and stop trying any of the others. This happens when it has found an action for which the expected reward is greater by a safe margin than its (possibly wrong) action value estimates for the remaining actions. What counts as a safe margin depends on how many steps have been taken (since the weighting for the weight up date decreases as the step count grows). From the plots, it seems that the fully greedy agent only identifies the optimal action around one third of the time and as a result the average reward per step saturates at around 1 (i.e. a 30% loss on the optimal agent).\n",
    "\n",
    "To conclude:\n",
    "* In the long run, average reward per step is maximised by an agent with an $\\varepsilon$ that is just greater than (but not equal to) zero. Its performance will tend to the optimal reward per step of around 1.5.\n",
    "* However, the smaller $\\varepsilon$ is set, the longer it will take for long-run conditions to be achieved.\n",
    "* Hence, for practical purposes (i.e. given any finite limit on the number of steps that will be taken) there will be an optimal value of $\\varepsilon$, which shrinks as the number of steps grows.\n",
    "* On the other hand, even a small amount of exploration (e.g. $\\varepsilon = 0.01$ in the plots) can quickly yield \"good\" rewards per step. Looking at the plots, although the $\\varepsilon = 0.01$ agent does not reliably identify the optimal action even after 10,000 steps, it does quickly identify some \"good enough\" actions: by around 1,500 steps, it is already exceeding the average reward per step of the $\\varepsilon = 0.1$ agent, despite not being as quick at identifying the truly optimal action.\n",
    "* Of course, by starting $\\varepsilon$ at a high number and shrinking it to zero, it is possible to get the best of both worlds: let the agent quickly identify the best action through extensive exploration and then let it always choose the optimal action by turning off exploration. However, this strategy would not work if the bandits were non-stationary (as discussed in later exercises).\n",
    "* The $\\varepsilon = 0$ fully greedy agent's performance saturates very quickly to a reward per step of around 1, because it never tries any other action once it has settled on action for which it has a good estimate of the value and that estimate exceeds its estimates for all the other actions (even if those other estimates are inaccurate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8843e-6a4c-4e41-8654-f5b8a913f99c",
   "metadata": {},
   "source": [
    "### Exercise 2.4\n",
    "\n",
    "Q: If the step-size parameters, $\\alpha_n$, are not constant, then the estimate $Q_n$ is a weighted average of previously received rewards with a weighting different from that given by (2.6). What is the weighting on each prior reward for the general case, analogous to (2.6), in terms of the sequence of step-size parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13357dd-7fe0-4cb0-81f1-e1deaba7b3ad",
   "metadata": {},
   "source": [
    "A: Recall the soft update rule for $Q_{n+1}$ — the value estimate for action $a$ to be used after this action has previously been taken $n$ times — is\n",
    "\n",
    "$$ Q_{n + 1} = Q_n + \\alpha_n \\left[ R_n - Q_n \\right]  $$\n",
    "\n",
    "where $R_n$ is the reward received the last time action $a$ was taken. We can then show by induction that, in analogy with equation (2.6) of the textbook,\n",
    "\n",
    "$$ Q_{n + 1} = \\prod_{i=1}^n (1 - \\alpha_i) Q_1 + \\sum_{i=1}^n \\alpha_i \\prod_{j=i+1}^n (1 - \\alpha_j) R_i,$$\n",
    "\n",
    "where we use the [empty product](https://en.wikipedia.org/wiki/Empty_product) convention, i.e. $\\prod_{i=n+1}^{n} p_i = 1$.\n",
    "\n",
    "*Proof*. Base case ($n = 1$):\n",
    "\n",
    "$$ Q_2 = (1 - \\alpha_1) Q_1 + \\alpha_1 R_1.$$\n",
    "\n",
    "Inductive step ($n \\to n+1$):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q_{n+1} &= (1 - \\alpha_n) \\left[ \\prod_{i=1}^{n-1} (1 - \\alpha_i) Q_1 + \\sum_{i=1}^{n-1} \\alpha_i \\prod_{j=i+1}^{n-1} (1 - \\alpha_j) R_i \\right]\n",
    "           + \\alpha_n R_n \\\\\n",
    "        &= \\prod_{i=1}^n (1 - \\alpha_i) Q_1 + \\sum_{i=1}^{n-1} \\alpha_i \\prod_{j=i+1}^{n} (1 - \\alpha_j) R_i + \\alpha_n R_n \\\\\n",
    "        &= \\prod_{i=1}^n (1 - \\alpha_i) Q_1 + \\sum_{i=1}^n \\alpha_i \\prod_{j=i+1}^n (1 - \\alpha_j) R_i.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babdff37-2e98-4966-87cd-2c69b67e9658",
   "metadata": {},
   "source": [
    "### Exercise 2.5\n",
    "\n",
    "Q: Design and conduct an experiment to demonstrate the difficulties that sample-average methods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the $q_*(a)$ start out equal and then take independent random walks... *(see textbook for remainder)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff1e720-b6c5-464c-a09b-da0e2a6a5d01",
   "metadata": {},
   "source": [
    "A: Instead of pure random walks as specified in the book, we add a little bit of mean reversion in our experiments. This prevents $q_*(a)$ growing without bound, helping ensure that the identity of the optimal action keeps changing around - providing a stiffer test of the difference between sample averaging and exponentially weighted averaging for estimating action values.\n",
    "\n",
    "The following code runs the experiment and plots the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba1b18-ce2d-4279-a756-967d4f5cf9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bandits import bandit_experiment, BanditResults\n",
    "from rl.environments.bandit import MultiArmedBandit\n",
    "from rl.agents import EpsilonGreedyRewardAveragingAgent\n",
    "from rl.learningrate import ConstantLearningRate\n",
    "\n",
    "\n",
    "# Define mean reverting varying bandit\n",
    "class MeanRevertingWalk:\n",
    "    def __init__(self, theta, vol, random_state=None):\n",
    "        self.theta = theta\n",
    "        self.vol = vol\n",
    "        self._rng = np.random.default_rng(random_state)\n",
    "        \n",
    "    def __call__(self, means, sigmas):\n",
    "        return (means + self._rng.normal(loc=-self.theta * means, scale=self.vol, size=means.shape)), sigmas\n",
    "\n",
    "\n",
    "def mean_reverting_varying_bandit(means, sigmas, theta, vol, random_state):\n",
    "    return MultiArmedBandit(\n",
    "        means,\n",
    "        sigmas,\n",
    "        state_updater=MeanRevertingWalk(theta, vol, random_state=random_state),\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "# Set up experiment parameters\n",
    "entropy = 5678942\n",
    "epsilon = 0.1\n",
    "n_levers = 10\n",
    "means, sigmas = [0.] * n_levers, [1.] * n_levers\n",
    "theta, vol = 0.001, 0.1\n",
    "agents_to_test = {\n",
    "    \"sample averaging\": partial(\n",
    "        EpsilonGreedyRewardAveragingAgent, epsilon, n_levers\n",
    "    ),\n",
    "    fr\"$\\alpha = 0.1$\": partial(\n",
    "        EpsilonGreedyRewardAveragingAgent, epsilon, n_levers, learning_rate_schedule=ConstantLearningRate(0.1)\n",
    "    ),\n",
    "}\n",
    "bandit_builder = partial(mean_reverting_varying_bandit, means, sigmas, theta, vol)\n",
    "n_steps = 10000\n",
    "test_bed_size = 200\n",
    "\n",
    "# Load saved results, or if not found, run experiment. Then plot results.\n",
    "results_file = f\"results/nonstationary-bandit-results-n_steps-{n_steps}-test_bed_size-{test_bed_size}-theta-{theta}-vol-{vol}.pkl\"\n",
    "try:\n",
    "    results = BanditResults.load(results_file)\n",
    "except FileNotFoundError:\n",
    "    results = bandit_experiment(\n",
    "        agents_to_test, bandit_builder, test_bed_size, n_steps, logging_period=10, entropy=entropy, n_jobs=-1, verbose=1\n",
    "    )\n",
    "    results.save(results_file)\n",
    "results.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14da7ce-ed58-4ed0-a162-a5142c9e96d0",
   "metadata": {},
   "source": [
    "From the charts we can clearly see that as time passes, sample averaging clearly suffers from bias: because it equally weights experiences from early in the simulation, its action value estimates only identify the correct optimal action around 20% of the time after a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08909fef-f785-4b35-9174-f9ad9b8c80da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
