{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f87bed5d-b9e0-457c-8795-c8a59386e523",
   "metadata": {},
   "source": [
    "# Chapter 2: Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da5c27-2dd1-4f02-8c02-54d3098ebe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2c159-3f5a-4838-ad15-d60339f5cfb0",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "Q: In $\\varepsilon$-greedy action selection, for the case of two actions and $\\varepsilon = 0.5$, what is the probability that the greedy action is selected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e02bf-4d51-48a5-a9fc-8369a64655fc",
   "metadata": {},
   "source": [
    "**A**: In $\\varepsilon$-greedy action selection, the greedy action be selected in one of two ways:\n",
    "1. With probability $1 - \\varepsilon$ it is chosen as the exploitation action;\n",
    "2. With probability $\\tfrac{\\varepsilon}{|\\mathcal{A}|}$ (where $\\mathcal{A}$ is the action space), it could be chosen at random as an exploration action.\n",
    "\n",
    "These are mutually exclusive events, so the total probability of the greedy action being chosen is\n",
    "\n",
    "$$1 - \\varepsilon + \\frac{\\varepsilon}{|\\mathcal{A}|}.$$\n",
    "\n",
    "In the question, $|\\mathcal{A}| = 2$ and $\\varepsilon = 0.5$. Hence the greedy action is chosen with probability\n",
    "\n",
    "$$1 - 0.5 + \\frac{0.5}{2} = 0.75.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b16e5c-0eb1-4e21-b204-d708dd6c0f6a",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "\n",
    "Q: Consider a $k$-armed bandit problem with $k=4$ actions, denoted 1, 2, 3 and 4. Consider applying to this problem a bandit algorithm using $\\varepsilon$-greedy action selection, sample-avarage action-value estimates, and initial estimates of $Q_1(a) = 0$ for all $a$. Suppose the initial sequence of actions and rewards is\n",
    "\n",
    "$$i$$ | $$A_i$$ | $$R_i$$\n",
    ":-: | :-: | :-:\n",
    " 1 | 1 | 1 \n",
    " 2 | 2 | 1\n",
    " 3 | 2 | 2\n",
    " 4 | 2 | 2\n",
    " 5 | 3 | 0\n",
    "\n",
    "On some of these times steps the $\\varepsilon$ case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12e614-3185-4405-b15c-6097eff2656e",
   "metadata": {},
   "source": [
    "A: We can manually calculate the action values as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec9866b-d9f2-47ad-9b89-a98cccb93c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data\n",
    "action_space = [1, 2, 3, 4]\n",
    "actions = [1, 2, 2, 2, 3]\n",
    "rewards = [1, 1, 2, 2, 0]\n",
    "\n",
    "# Set up initial Q values and action counts N for i=1\n",
    "Q = {a: 0 for a in action_space}\n",
    "N = {a: 0 for a in action_space}\n",
    "\n",
    "# Iteratively update N and Q for remaining time steps and save Q history\n",
    "Q_history = []\n",
    "for i, (A, R) in enumerate(zip(actions, rewards), 1):\n",
    "    Q_history.append(Q.copy())\n",
    "    N[A] += 1\n",
    "    Q[A] = Q[A] + (1.0 / N[A]) * (R - Q[A])\n",
    "\n",
    "# Display table of action values\n",
    "(\n",
    "    pd.DataFrame.from_records(Q_history, index=range(1, len(actions) + 1))\n",
    "    .rename(lambda a:f\"$$Q({a})$$\", axis=1)\n",
    "    .rename_axis(\"$$i$$\", axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7aa1c3-a7b0-45fb-b44f-1c0b46e272f1",
   "metadata": {},
   "source": [
    "From these we can determine whether a greedy action was chosen at each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a8136-8683-49b4-9097-d8546eeef166",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_action_values = [Q[A] for Q, A in zip(Q_history, actions)]\n",
    "greedy_action_values = [max(Q.values()) for Q in Q_history]\n",
    "definitely_exploration = [c != g for c, g in zip(chosen_action_values, greedy_action_values)]\n",
    "\n",
    "# Display results as a table\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"$$\\max_{a \\in \\mathcal{A}} Q_i(a)$$\": greedy_action_values,\n",
    "        \"$$Q_i(A_i)$$\": chosen_action_values,\n",
    "        \"Definitely exploration?\": definitely_exploration,\n",
    "    },\n",
    "    index=pd.Index(range(1, len(actions) + 1), name=\"$$i$$\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eba803-8949-4ecc-a9e1-0f4b189b4bed",
   "metadata": {},
   "source": [
    "So we can see that at timesteps $i=2$ and $i=5$, the chosen action was **not** the greedy action and hence exploration (the \"$\\varepsilon$ case\") definitely happened. For all other time steps, we do not know: perhaps the greedy action was chosen \"on purpose\", i.e. as an exploitation action, or perhaps it was still chosen as an exploration action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e2af57-7688-4a19-a9f6-ed6572ba5443",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "\n",
    "Q: In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97be395-b830-41bc-906f-a676f3e9574e",
   "metadata": {},
   "source": [
    "A: Figure 2.2 from the textbook shows average learning curves (over 2000 randomly chosen bandits) for $\\varepsilon$-greedy agents with $\\varepsilon = 0$, $0.01$ and $0.1$.\n",
    "\n",
    "Let's begin by first reproducing this figure. To save computation time, we use a testbed of only 200 bandits, but extend the simulation length to 3000 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b52cf2-3af7-416e-8294-a7f8859744ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bandits import get_epsilon_greedy_bandit_agent_builder, bandit_experiment, BanditResults\n",
    "\n",
    "# Set up experiment parameters\n",
    "n_levers = 10\n",
    "agents_to_test = {\n",
    "    fr\"$\\varepsilon = {epsilon}$\": get_epsilon_greedy_bandit_agent_builder(epsilon, n_levers)\n",
    "    for epsilon in [0.0, 0.01, 0.1]\n",
    "}\n",
    "n_steps = 1000\n",
    "test_bed_size = 200\n",
    "rng = np.random.default_rng(238402285)\n",
    "\n",
    "# Load saved results, or if not found, run experiment. Then plot results.\n",
    "results_file = f\"results/epsilon-bandit-results-n_steps-{n_steps}-test_bed_size-{test_bed_size}.pkl\"\n",
    "try:\n",
    "    results = BanditResults.load(results_file)\n",
    "except FileNotFoundError:\n",
    "    %time results = bandit_experiment(agents_to_test, test_bed_size, n_steps, random_state=rng)\n",
    "    results.save(results_file)\n",
    "results.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de8d90-1f1d-49ba-9e8c-34ddc9829e13",
   "metadata": {},
   "source": [
    "We now return to answering the question.\n",
    "\n",
    "Let's start with the case where $\\varepsilon > 0$. In this case, as the simulation length goes to infinity, *all* states will be explored an infinite number of times and hence the agent's action value estimates for all states will converge to the true values. This means that, in the long run, the average reward per step for an $\\varepsilon$-greedy agent with non-zero $\\varepsilon$ will ultimately converge to\n",
    "\n",
    "$$(1 - \\varepsilon)  R_\\mathrm{opt} + \\varepsilon R_\\mathrm{random}$$\n",
    "\n",
    "where $R_\\mathrm{opt}$ is the expected reward-per-step for an agent that always selects the optimal action and $R_\\mathrm{random}$ is the expected reward-per-step for an agent that always selects a random action. When we average over a large number of random bandits (whose mean rewards are drawn from a standard normal distribution and whose standard deviations are one, as specified in the book), then $R_\\mathrm{random} = 0$ and\n",
    "\n",
    "$$R_\\mathrm{opt} = \\mathbb{E}\\left[\\max(Z_1, Z_2, \\ldots, Z_{10})\\right]$$\n",
    "\n",
    "where $Z_1, \\ldots, Z_{10}$ are independent standard normal random variables. The pdf for the distribution for the max of these variables can be derived and the expectation numerically estimated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176a42f-7d9b-4943-9689-7d5d175bc6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.average_optimal_action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a9a70-814b-4bc6-b439-bdea28dd7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.integrate import quadrature\n",
    "\n",
    "integrand = lambda z: 10. * z * (norm.cdf(z) ** 9) * norm.pdf(z)\n",
    "print(\"Expected reward for perfect agent: {:.3f} (error: {:.1e})\".format(*quadrature(integrand, -10, 10, maxiter=1000)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dac536a-fcc3-40d5-98e8-dc147845c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8 * 1.54"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
