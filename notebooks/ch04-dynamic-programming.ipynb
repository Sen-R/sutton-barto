{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ee2fb0-6587-4cd4-8800-ddd707aa808e",
   "metadata": {},
   "source": [
    "# Chapter 4: Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893bb12-1eed-4147-b0a2-ae30453b5e6d",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "\n",
    "Q: In Example 4.1, if $\\pi$ is the equiprobable random policy, what is $q_\\pi(11, \\texttt{down})$? What is $q_\\pi(7, \\texttt{down})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09cfbc5-2ad4-40ac-b067-2bf2cfc6a56f",
   "metadata": {},
   "source": [
    "A: Example 4.1 describes a 4-by-4 gridworld where the top-left and bottom-right cells are terminal states. Each transition has reward -1 (including invalid actions) and there is no discounting. The (non-terminal) states are numbered left-to-right, then top-to-bottom, starting from 1 for state $(0, 1)$. Since there is no discounting, the (absolute) value of any state corresponds to the expected number of steps it will take to reach a terminal state from that state under the current policy.\n",
    "\n",
    "Let us start by specifying this gridworld as code and deriving, using iterative policy evaluation, the state value function $v_\\pi$ for this policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d09a15-099b-453d-a656-18875cac6961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "import numpy as np\n",
    "from dp import GridWorld\n",
    "from dp.solve import iterative_policy_evaluation, StateValueFunction\n",
    "\n",
    "# Gridworld specified in Example 4.1 of Sutton, Barto (2018)\n",
    "gridworld_4_1 = GridWorld(\n",
    "    size=4,\n",
    "    terminal_states=[(0, 0), (3, 3)],\n",
    "    default_move_reward=-1.0,\n",
    "    invalid_action_reward = -1.0,\n",
    ")\n",
    "\n",
    "# Evaluate equiprobably random policy\n",
    "v = StateValueFunction(gridworld_4_1)\n",
    "def pi(state: tuple[int, int]) -> list[tuple[str, float]]:\n",
    "    return [(a, 0.25) for a in gridworld_4_1.actions(state)]\n",
    "iterative_policy_evaluation(v, pi, 1.0)\n",
    "\n",
    "# Check against the result in the textbook\n",
    "desired_v = np.array([\n",
    "    [0.0, -14.0, -20.0, -22.0],\n",
    "    [-14.0, -18.0, -20.0, -20.0],\n",
    "    [-20.0, -20.0, -18.0, -14.0],\n",
    "    [-22.0, -20.0, -14.0, 0.0]\n",
    "]).ravel()\n",
    "assert_almost_equal(\n",
    "    actual=list(v.to_dict().values()),\n",
    "    desired=desired_v,\n",
    "    decimal=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df7f0f-5f3d-49a5-91cd-f4e936f657ed",
   "metadata": {},
   "source": [
    "Once we know $v_\\pi$, we can compute action values $q_\\pi(s, a)$ using the relationship:\n",
    "\n",
    "$$q_\\pi(s, a) = \\mathbb{E}_\\pi\\left[R_{t+1} + \\gamma\\,v_\\pi(S_{t+1}) \\mid S_t=s, A_t=a\\right].$$\n",
    "\n",
    "This means, by manual computation (and with $\\gamma = 1$):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\pi(11, \\texttt{down}) &= -1\\\\\n",
    "q_\\pi(7, \\texttt{down}) &= -15.\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can check these results against numerically computing these action values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365745e1-1726-49c5-9d7e-93c93b33399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_almost_equal(\n",
    "    actual=v.backup_action_value(gridworld_4_1.i2s(11), \"s\", gamma=1.0),\n",
    "    desired=-1.0,\n",
    "    decimal=2,\n",
    ")\n",
    "assert_almost_equal(\n",
    "    actual=v.backup_action_value(gridworld_4_1.i2s(7), \"s\", gamma=1.0),\n",
    "    desired=-15.0,\n",
    "    decimal=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf56b6e-70e0-4226-840e-a177a7875ef1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 4.2\n",
    "\n",
    "Q: In example 4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, $\\texttt{left}$, $\\texttt{up}$, $\\texttt{right}$, and $\\texttt{down}$, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions *from* the original states are unchanged. What, then, is $v_\\pi(15)$ for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action $\\texttt{down}$ from state 13 takes the agent to the new state 15. What is $v_\\pi(15)$ for the equiprobable random policy in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e50088a-1df1-45ce-b093-20d851eed021",
   "metadata": {},
   "source": [
    "A: For the first part of the question, we assume transitions from the original states are unchanged. This means that state 15 is not reachable from any of the original states and therefore has no impact on the original states' values. Since\n",
    "\n",
    "$$v_\\pi(15) = -1 + \\tfrac{v_\\pi(12) + v_\\pi(13) + v_\\pi(14) + v_\\pi(15)}{4},$$\n",
    "\n",
    "we can rearrange and insert the original state values for states 12, 13 and 14 to find\n",
    "\n",
    "$$v_\\pi(15) = \\frac{4}{3} \\, \\left(-1 + \\tfrac{-22 -20 -14}{4}\\right) = -20.$$\n",
    "\n",
    "In the second part of the question, we suppose that going down from state 13 moves the agent to state 15 and are against asked for the value of state 15. We can tackle this by performing iterative policy evaluation, starting from $v_\\pi$ as given in the first part of the question. We begin by updating the value of $v_\\pi(13)$ but immediately notice that this won't change since (in the first part of the question) $v_\\pi(15) = v_\\pi(13)$. Therefore we see that none of the other $v_\\pi(s)$ will need updating either, i.e. we have already converged. In other words, introducing this transition makes no difference to $v_\\pi(15)$ for the equiprobably random policy: it is still -20.\n",
    "\n",
    "Let's check these answers against a numerical solution. First let's define a `FiniteMDP` class to represent the modified gridworld problem in this exercise. It encapsulates a standard `GridWorld` but augments it to include custom states and transitions, plus uses the state numbering scheme\n",
    "provided in the exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a31b7f-62d2-45bc-ae7a-39546359820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from dp import FiniteMDP\n",
    "\n",
    "class ModifiedGW(FiniteMDP):\n",
    "    \"\"\"Gridworld with added states and transitions,\n",
    "    as per exercise 4.2 of Sutton, Barto (2018).\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, custom_states_and_transitions):\n",
    "        self._inner_gw = GridWorld(\n",
    "            4,\n",
    "            terminal_states=((0, 0), (3, 3)),\n",
    "            default_move_reward=-1.0,\n",
    "            invalid_action_reward=-1.0,\n",
    "        )\n",
    "        self.custom_states_and_transitions = custom_states_and_transitions\n",
    "    \n",
    "    def mgw2gw_state(self, mgw_state):\n",
    "        \"\"\"Translates states (1 to 15) in the exercise to original\n",
    "        GridWorld states (tuples of ints), where possible. For custom\n",
    "        states not in the original gridworld, returns None.\"\"\"\n",
    "        if mgw_state in self.custom_states_and_transitions.keys():\n",
    "            return None\n",
    "        else:\n",
    "            return self._inner_gw.i2s(mgw_state)\n",
    "    \n",
    "    def gw2mgw_state(self, gw_state):\n",
    "        \"\"\"Translates original GridWorld states (tuples of ints) to\n",
    "        the states numbered as per the exercise. Note, 0 represents\n",
    "        both the terminal states.\"\"\"\n",
    "        if gw_state in self._inner_gw.terminal_states:\n",
    "            return 0\n",
    "        else:\n",
    "            return self._inner_gw.s2i(gw_state)\n",
    "    \n",
    "    @property\n",
    "    def states(self):\n",
    "        return sorted(\n",
    "            set(range(0, 15)) | self.custom_states_and_transitions.keys()\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def actions(self, state):\n",
    "        return (\"n\", \"e\", \"w\", \"s\")\n",
    "    \n",
    "    def next_states_and_rewards(self, state: int, action: str):\n",
    "        if state in self.custom_states_and_transitions.keys():\n",
    "            next_state = self.custom_states_and_transitions[state][action]\n",
    "            reward = -1\n",
    "        else:\n",
    "            (nss, ps), reward = self._inner_gw.next_states_and_rewards(\n",
    "                self.mgw2gw_state(state), action\n",
    "            )\n",
    "            assert len(nss) == 1\n",
    "            assert ps[0] == 1.0\n",
    "            next_state = self.gw2mgw_state(nss[0])\n",
    "        return ((next_state,), (1.0,),), reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba44fd1-890f-4a03-afa2-40c92d619049",
   "metadata": {},
   "source": [
    "Now we answer the first part of the question, numerically, by adding a custom state 15, but keeping all other state transitions as they were before.\n",
    "We then check that an equiprobably random policy yields exactly the same $v_\\pi$ as the original problem for original states, as well as a value\n",
    "of -20 for state 15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c9448-a5d8-4840-a73b-b1f9b701d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up modified gridworld and run iterative policy evaluation\n",
    "v_modified_1 = StateValueFunction(\n",
    "    ModifiedGW(\n",
    "        {15: {\"w\": 12, \"n\": 13, \"e\": 14, \"s\": 15}}\n",
    "    )\n",
    ")\n",
    "iterative_policy_evaluation(v_modified_1, pi, gamma=1.0)\n",
    "\n",
    "# Check if state values are as expected\n",
    "for s in range(0, 14):\n",
    "    assert_almost_equal(v_modified_1(s), desired_v[s], decimal=2)\n",
    "assert_almost_equal(v_modified_1(15), -20.0, decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef616fc-909e-42f3-b414-49eb65c099d6",
   "metadata": {},
   "source": [
    "Now we do the same thing for the second part of the question, this time adding custom transitions to state 13 as well as a new state 15. Again we should find the same state value function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9dd19-23dd-44a2-91c8-ef24ccc40402",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_modified_2 = StateValueFunction(\n",
    "    ModifiedGW(\n",
    "        {\n",
    "            13: {\"w\": 12, \"n\": 9, \"e\": 14, \"s\": 15},\n",
    "            15: {\"w\": 12, \"n\": 13, \"e\": 14, \"s\": 15},\n",
    "        },\n",
    "    )\n",
    ")\n",
    "iterative_policy_evaluation(v_modified_2, pi, gamma=1.0)\n",
    "\n",
    "for s in range(0, 16):\n",
    "    assert_almost_equal(v_modified_2(s), v_modified_1(s), decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff7711-23a9-4a44-94cf-b94b7642549f",
   "metadata": {},
   "source": [
    "### Exercise 4.3\n",
    "\n",
    "Q: What are the equations analogous to (4.3), (4.4), and (4.5) for the action-value function $q_\\pi$ and its successive approximation by a sequence of functions $q_0$, $q_1$, $q_2$, ...?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c0e7e-26d4-461e-8004-cbba0995c5df",
   "metadata": {},
   "source": [
    "A: The equations referenced in the question develop the iterative policy evaluation algorithm for approximating the state-value function $v_\\pi$. To do the same for the action-value function $q_\\pi$, we begin with the Bellman equation for the action-value function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\pi(s, a) &\\doteq \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma \\, q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t=s, A_t=a\\right] \\\\\n",
    "&= \\sum_{s' \\in \\mathcal{S}} \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') \\, p(s' | s, a) \\left[r(s, a, s') + \\gamma\\,q_\\pi(s', a') \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This equation is satisfied by a fixed point of the following iteration, leading directly to a iterative policy evaluation algorithm that can be applied directly to action-values:\n",
    "\n",
    "$$\n",
    "q_{i+1}(s, a) \\doteq \\sum_{s' \\in \\mathcal{S}} \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') \\, p(s' | s, a) \\left[r(s, a, s') + \\gamma\\,q_i(s', a') \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5731c916-7139-4f83-bc90-33f1332a3ec3",
   "metadata": {},
   "source": [
    "### Exercise 4.4\n",
    "\n",
    "Q: The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so that convergence is guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d6015-f761-4db0-b0cb-a10ab078c1e2",
   "metadata": {},
   "source": [
    "A: The scenario described in the question can occur if there are states where two or more actions have the same action value. If different such actions can be selected for different iterations then *policy-stable* can end up remaining false even though the algorithm has actually converged. To fix this, we can update the *if* condition for determining whether *policy-stable* should be set to false. In the pseudocode, *policy-stable* is set to false whenever *old-action* differs from $\\pi(s)$; instead the condition should be whether the action-value corresponding to $old-action$ differs from the action-value for $\\pi(s)$. In practice (and in the implementation in the `dp` package), due to numerical error, we should check whether the action-values for the old and new policy actions are close enough (within some tolerance); where this is the case, we should not set *policy-stable* to false, as that iteration of the update has not resulted in a better policy (within tolerance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf0d8c5-a71b-400c-b178-7b5b0bba3b48",
   "metadata": {},
   "source": [
    "### Exercise 4.5\n",
    "\n",
    "Q: How would policy iteration be defined for action values? Give a complete algorithm for computing $q_*$, analogous to that on page 80 for computing $v_*$. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
