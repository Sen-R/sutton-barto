{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ee2fb0-6587-4cd4-8800-ddd707aa808e",
   "metadata": {},
   "source": [
    "# Chapter 4: Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893bb12-1eed-4147-b0a2-ae30453b5e6d",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "\n",
    "Q: In Example 4.1, if $\\pi$ is the equiprobable random policy, what is $q_\\pi(11, \\texttt{down})$? What is $q_\\pi(7, \\texttt{down})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09cfbc5-2ad4-40ac-b067-2bf2cfc6a56f",
   "metadata": {},
   "source": [
    "A: Example 4.1 describes a 4-by-4 gridworld where the top-left and bottom-right cells are terminal states. Each transition has reward -1 (including invalid actions) and there is no discounting. The (non-terminal) states are numbered left-to-right, then top-to-bottom, starting from 1 for state $(0, 1)$. Since there is no discounting, the (absolute) value of any state corresponds to the expected number of steps it will take to reach a terminal state from that state under the current policy.\n",
    "\n",
    "Let us start by specifying this gridworld as code and deriving, using iterative policy evaluation, the state value function $v_\\pi$ for this policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d09a15-099b-453d-a656-18875cac6961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "import numpy as np\n",
    "from dp import GridWorld\n",
    "from dp.solve import iterative_policy_evaluation, StateValueFunction\n",
    "\n",
    "# Gridworld specified in Example 4.1 of Sutton, Barto (2018)\n",
    "gridworld_4_1 = GridWorld(\n",
    "    size=4,\n",
    "    terminal_states=[(0, 0), (3, 3)],\n",
    "    default_move_reward=-1.0,\n",
    "    invalid_action_reward = -1.0,\n",
    ")\n",
    "\n",
    "# Evaluate equiprobably random policy\n",
    "v = StateValueFunction(gridworld_4_1)\n",
    "def pi(state: tuple[int, int]) -> list[tuple[str, float]]:\n",
    "    return [(a, 0.25) for a in gridworld_4_1.actions(state)]\n",
    "iterative_policy_evaluation(v, pi, 1.0)\n",
    "\n",
    "# Check against the result in the textbook\n",
    "desired_v = np.array([\n",
    "    [0.0, -14.0, -20.0, -22.0],\n",
    "    [-14.0, -18.0, -20.0, -20.0],\n",
    "    [-20.0, -20.0, -18.0, -14.0],\n",
    "    [-22.0, -20.0, -14.0, 0.0]\n",
    "]).ravel()\n",
    "assert_almost_equal(\n",
    "    actual=list(v.to_dict().values()),\n",
    "    desired=desired_v,\n",
    "    decimal=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df7f0f-5f3d-49a5-91cd-f4e936f657ed",
   "metadata": {},
   "source": [
    "Once we know $v_\\pi$, we can compute action values $q_\\pi(s, a)$ using the relationship:\n",
    "\n",
    "$$q_\\pi(s, a) = \\mathbb{E}_\\pi\\left[R_{t+1} + \\gamma\\,v_\\pi(S_{t+1}) \\mid S_t=s, A_t=a\\right].$$\n",
    "\n",
    "This means, by manual computation (and with $\\gamma = 1$):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\pi(11, \\texttt{down}) &= -1\\\\\n",
    "q_\\pi(7, \\texttt{down}) &= -15.\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can check these results against numerically computing these action values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365745e1-1726-49c5-9d7e-93c93b33399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_almost_equal(\n",
    "    actual=v.backup_action_value(gridworld_4_1.i2s(11), \"s\", gamma=1.0),\n",
    "    desired=-1.0,\n",
    "    decimal=2,\n",
    ")\n",
    "assert_almost_equal(\n",
    "    actual=v.backup_action_value(gridworld_4_1.i2s(7), \"s\", gamma=1.0),\n",
    "    desired=-15.0,\n",
    "    decimal=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf56b6e-70e0-4226-840e-a177a7875ef1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 4.2\n",
    "\n",
    "Q: In example 4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, $\\texttt{left}$, $\\texttt{up}$, $\\texttt{right}$, and $\\texttt{down}$, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions *from* the original states are unchanged. What, then, is $v_\\pi(15)$ for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action $\\texttt{down}$ from state 13 takes the agent to the new state 15. What is $v_\\pi(15)$ for the equiprobable random policy in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e50088a-1df1-45ce-b093-20d851eed021",
   "metadata": {},
   "source": [
    "A: For the first part of the question, we assume transitions from the original states are unchanged. This means that state 15 is not reachable from any of the original states and therefore has no impact on the original states' values. Since\n",
    "\n",
    "$$v_\\pi(15) = -1 + \\tfrac{v_\\pi(12) + v_\\pi(13) + v_\\pi(14) + v_\\pi(15)}{4},$$\n",
    "\n",
    "we can rearrange and insert the original state values for states 12, 13 and 14 to find\n",
    "\n",
    "$$v_\\pi(15) = \\frac{4}{3} \\, \\left(-1 + \\tfrac{-22 -20 -14}{4}\\right) = -20.$$\n",
    "\n",
    "In the second part of the question, we suppose that going down from state 13 moves the agent to state 15 and are against asked for the value of state 15. We can tackle this by performing iterative policy evaluation, starting from $v_\\pi$ as given in the first part of the question. We begin by updating the value of $v_\\pi(13)$ but immediately notice that this won't change since (in the first part of the question) $v_\\pi(15) = v_\\pi(13)$. Therefore we see that none of the other $v_\\pi(s)$ will need updating either, i.e. we have already converged. In other words, introducing this transition makes no difference to $v_\\pi(15)$ for the equiprobably random policy: it is still -20.\n",
    "\n",
    "Let's check these answers against a numerical solution. First let's define a `FiniteMDP` class to represent the modified gridworld problem in this exercise. It encapsulates a standard `GridWorld` but augments it to include custom states and transitions, plus uses the state numbering scheme\n",
    "provided in the exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a31b7f-62d2-45bc-ae7a-39546359820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from dp import FiniteMDP\n",
    "\n",
    "class ModifiedGW(FiniteMDP):\n",
    "    \"\"\"Gridworld with added states and transitions,\n",
    "    as per exercise 4.2 of Sutton, Barto (2018).\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, custom_states_and_transitions):\n",
    "        self._inner_gw = GridWorld(\n",
    "            4,\n",
    "            terminal_states=((0, 0), (3, 3)),\n",
    "            default_move_reward=-1.0,\n",
    "            invalid_action_reward=-1.0,\n",
    "        )\n",
    "        self.custom_states_and_transitions = custom_states_and_transitions\n",
    "    \n",
    "    def mgw2gw_state(self, mgw_state):\n",
    "        \"\"\"Translates states (1 to 15) in the exercise to original\n",
    "        GridWorld states (tuples of ints), where possible. For custom\n",
    "        states not in the original gridworld, returns None.\"\"\"\n",
    "        if mgw_state in self.custom_states_and_transitions.keys():\n",
    "            return None\n",
    "        else:\n",
    "            return self._inner_gw.i2s(mgw_state)\n",
    "    \n",
    "    def gw2mgw_state(self, gw_state):\n",
    "        \"\"\"Translates original GridWorld states (tuples of ints) to\n",
    "        the states numbered as per the exercise. Note, 0 represents\n",
    "        both the terminal states.\"\"\"\n",
    "        if gw_state in self._inner_gw.terminal_states:\n",
    "            return 0\n",
    "        else:\n",
    "            return self._inner_gw.s2i(gw_state)\n",
    "    \n",
    "    @property\n",
    "    def states(self):\n",
    "        return sorted(\n",
    "            set(range(0, 15)) | self.custom_states_and_transitions.keys()\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def actions(self, state):\n",
    "        return (\"n\", \"e\", \"w\", \"s\")\n",
    "    \n",
    "    def next_states_and_rewards(self, state: int, action: str):\n",
    "        if state in self.custom_states_and_transitions.keys():\n",
    "            next_state = self.custom_states_and_transitions[state][action]\n",
    "            reward = -1\n",
    "        else:\n",
    "            (nss, ps), reward = self._inner_gw.next_states_and_rewards(\n",
    "                self.mgw2gw_state(state), action\n",
    "            )\n",
    "            assert len(nss) == 1\n",
    "            assert ps[0] == 1.0\n",
    "            next_state = self.gw2mgw_state(nss[0])\n",
    "        return ((next_state,), (1.0,),), reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba44fd1-890f-4a03-afa2-40c92d619049",
   "metadata": {},
   "source": [
    "Now we answer the first part of the question, numerically, by adding a custom state 15, but keeping all other state transitions as they were before.\n",
    "We then check that an equiprobably random policy yields exactly the same $v_\\pi$ as the original problem for original states, as well as a value\n",
    "of -20 for state 15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c9448-a5d8-4840-a73b-b1f9b701d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up modified gridworld and run iterative policy evaluation\n",
    "v_modified_1 = StateValueFunction(\n",
    "    ModifiedGW(\n",
    "        {15: {\"w\": 12, \"n\": 13, \"e\": 14, \"s\": 15}}\n",
    "    )\n",
    ")\n",
    "iterative_policy_evaluation(v_modified_1, pi, gamma=1.0)\n",
    "\n",
    "# Check if state values are as expected\n",
    "for s in range(0, 14):\n",
    "    assert_almost_equal(v_modified_1(s), desired_v[s], decimal=2)\n",
    "assert_almost_equal(v_modified_1(15), -20.0, decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef616fc-909e-42f3-b414-49eb65c099d6",
   "metadata": {},
   "source": [
    "Now we do the same thing for the second part of the question, this time adding custom transitions to state 13 as well as a new state 15. Again we should find the same state value function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9dd19-23dd-44a2-91c8-ef24ccc40402",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_modified_2 = StateValueFunction(\n",
    "    ModifiedGW(\n",
    "        {\n",
    "            13: {\"w\": 12, \"n\": 9, \"e\": 14, \"s\": 15},\n",
    "            15: {\"w\": 12, \"n\": 13, \"e\": 14, \"s\": 15},\n",
    "        },\n",
    "    )\n",
    ")\n",
    "iterative_policy_evaluation(v_modified_2, pi, gamma=1.0)\n",
    "\n",
    "for s in range(0, 16):\n",
    "    assert_almost_equal(v_modified_2(s), v_modified_1(s), decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff7711-23a9-4a44-94cf-b94b7642549f",
   "metadata": {},
   "source": [
    "### Exercise 4.3\n",
    "\n",
    "Q: What are the equations analogous to (4.3), (4.4), and (4.5) for the action-value function $q_\\pi$ and its successive approximation by a sequence of functions $q_0$, $q_1$, $q_2$, ...?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c0e7e-26d4-461e-8004-cbba0995c5df",
   "metadata": {},
   "source": [
    "A: The equations referenced in the question develop the iterative policy evaluation algorithm for approximating the state-value function $v_\\pi$. To do the same for the action-value function $q_\\pi$, we begin with the Bellman equation for the action-value function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\pi(s, a) &\\doteq \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma \\, q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t=s, A_t=a\\right] \\\\\n",
    "&= \\sum_{s' \\in \\mathcal{S}} \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') \\, p(s' | s, a) \\left[r(s, a, s') + \\gamma\\,q_\\pi(s', a') \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This equation is satisfied by a fixed point of the following iteration, leading directly to a iterative policy evaluation algorithm that can be applied directly to action-values:\n",
    "\n",
    "$$\n",
    "q_{i+1}(s, a) \\doteq \\sum_{s' \\in \\mathcal{S}} \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') \\, p(s' | s, a) \\left[r(s, a, s') + \\gamma\\,q_i(s', a') \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5731c916-7139-4f83-bc90-33f1332a3ec3",
   "metadata": {},
   "source": [
    "### Exercise 4.4\n",
    "\n",
    "Q: The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so that convergence is guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d6015-f761-4db0-b0cb-a10ab078c1e2",
   "metadata": {},
   "source": [
    "A: The scenario described in the question can occur if there are states where two or more actions have the same action value. If different such actions can be selected for different iterations then *policy-stable* can end up remaining false even though the algorithm has actually converged. To fix this, we can update the *if* condition for determining whether *policy-stable* should be set to false. In the pseudocode, *policy-stable* is set to false whenever *old-action* differs from $\\pi(s)$; instead the condition should be whether the action-value corresponding to $old-action$ differs from the action-value for $\\pi(s)$. In practice (and in the implementation in the `dp` package), due to numerical error, we should check whether the action-values for the old and new policy actions are close enough (within some tolerance); where this is the case, we should not set *policy-stable* to false, as that iteration of the update has not resulted in a better policy (within tolerance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf0d8c5-a71b-400c-b178-7b5b0bba3b48",
   "metadata": {},
   "source": [
    "### Exercise 4.5\n",
    "\n",
    "Q: How would policy iteration be defined for action values? Give a complete algorithm for computing $q_*$, analogous to that on page 80 for computing $v_*$. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c1b56-582c-4c89-8e76-c51b623743a2",
   "metadata": {},
   "source": [
    "A: At a high level, the structure of policy iteration stays the same: following initialisation, the algorithm alternates between policy evaluation and policy improvement. The differences are in the details of how policy evaluation and policy improvement proceed:\n",
    "\n",
    "* Policy evaluation is now about determining $q_\\pi$ rather than $v_\\pi$ for the current policy. For a MDP where the dynamics are known, this is probably most efficiently achieved by performing policy evaluation for $v_\\pi$ as before, and then computing $q_\\pi$ using the relationship:\n",
    "  $$ q_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\left[r(s, a, s') + \\gamma \\, v_\\pi(s')\\right].$$\n",
    "  On the other hand, for more typical RL-type problems, policy evaluation may proceed by directly estimating $q_\\pi$ itself using Monte Carlo or one of the techniques covered in the textbook for estimating action-values based on previous examples.\n",
    "  \n",
    "* Policy iteration is similar to before: it involves looping over all states, searching each time for an action that may have a higher value than the current policy's suggested action. Working with $q_\\pi$ instead of $v_\\pi$ means that the policy update equation within this loop simplifies to\n",
    "  $$ \\pi(s) \\leftarrow \\argmax_{a} q_\\pi(s, a).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bf766d-cfad-49bb-a8de-866f4dd3023c",
   "metadata": {},
   "source": [
    "### Exercise 4.6\n",
    "\n",
    "Q: Suppose you are restricted to considering only policies that are $\\varepsilon$-soft, meaning that the probability of selecting each action in each state, $s$, is at least $\\varepsilon / |\\mathcal{A}(s)|$. Describe qualititatively the changes that would be required in each of the steps 3, 2, and 1, in that order of the policy iteration algorithm for $v_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ddfb6-98ec-419a-9d1b-353380953a39",
   "metadata": {},
   "source": [
    "A: Firstly, note that we could use standard policy iteration to solve this problem if we define a modified MDP, with transitions modified with respect to the original MDP. We just need to define state transitions under the modified MDP so that, when we (deterministically) choose action $a$ in state $s$ under the modified MDP, this should result in a state transition distribution equivalent to taking an $\\varepsilon$-soft action $a$ in state $s$ under the original MDP.\n",
    "\n",
    "However, let's now answer the question as instructed. Firstly, looking at step 3 (the policy improvement step), we see that there needs to be no modification to this step in the algorithm. This is because the best $\\varepsilon$-soft action in a given state must always coincide with the best deterministic action in the same state. We can see this by noting that the value of taking $\\varepsilon$-soft action $a$ in state $s$ is given\n",
    "by the expression\n",
    "\n",
    "$$\n",
    "q^{\\varepsilon\\text{-soft}}_\\pi (s, a) \\doteq (1 - \\varepsilon) \\sum_{s' \\in \\mathcal{S}} p(s | s, a) \\left[r(s, a, s') + \\gamma \\, v_\\pi(s') \\right] + \\tfrac{\\varepsilon}{|\\mathcal{A}(s)|} \\sum_{s' \\in \\mathcal{S}} \\sum_{a' \\in \\mathcal{A}(s)} \\left[r(s, a', s') + \\gamma\\,v_\\pi(s')\\right] .\n",
    "$$\n",
    "\n",
    "Note that the second term on the righthand side is independent of $a$. Hence,\n",
    "$$\\argmax_a q^{\\varepsilon\\text{-soft}}_\\pi (s, a) = \\argmax_a \\sum_{s' \\in \\mathcal{S}} p(s | s, a) \\left[r(s, a, s') + \\gamma \\, v_\\pi(s') \\right]$$\n",
    "as in the original algorithm.\n",
    "\n",
    "Next we turn to step 2, the policy evaluation step. Here, we need to modify the update step for the state-value function as follows, in order to account for the possibility of the agent taking a random action rather than the desired action $a$:\n",
    "\n",
    "$$v_\\pi(s) \\leftarrow \\sum_{s' \\in \\mathcal{S}} \\sum_{a' \\in \\mathcal{A}(s)} \\tilde{p}_\\pi(a' | s) \\, p(s' | s, a) \\left[r(s, a', s') + \\gamma\\,v_\\pi(s)\\right],$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\tilde{p}_\\pi(a' | s) | \\doteq \\tfrac{\\varepsilon}{|\\mathcal{A}(s)|} + (1 - \\varepsilon) \\, \\mathbb{1}_{a' = \\pi(s)}.$$\n",
    "\n",
    "No changes are required to the initialisation step (step 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c02c75-5bd2-4996-97a3-102adec79fdc",
   "metadata": {},
   "source": [
    "### Exercise 4.7\n",
    "\n",
    "Q: Write a program for policy iteration and re-solve Jack's car rental problem with the following changes. One of Jack's employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs \\\\$2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), then an additional cost of \\\\$4 must be incurred to use a second parking lot (independent of how many cars are kept there)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a25bc-d26c-4ada-b054-c24342efdfbc",
   "metadata": {},
   "source": [
    "A: Jack's car rental is implemented in the `dp` library. One of the test cases in the associated test suite checks that solving for this MDP yields the same optimal policy given in the textbook. Here, we extend the base problem to cater for this modified reward function and use policy iteration to find the new optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04aeae-86fe-407c-a68f-c60632fc7e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dp import StateValueFunction\n",
    "from dp.jackscarrental import JacksCarRental\n",
    "from dp.solve import policy_iteration\n",
    "\n",
    "class ModifiedJCR(JacksCarRental):\n",
    "    \"\"\"Modified version of Jack's car rental problem, as described in Exercise\n",
    "    4.7 of the textbook.\"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialise base class with problem parameters given in the textbook\n",
    "        super().__init__(\n",
    "            capacity=20,\n",
    "            overnight_moves_limit=5,\n",
    "            exp_demand_per_location=(3, 4),\n",
    "            exp_returns_per_location=(3, 2),\n",
    "            reward_for_rental=10.0,\n",
    "            reward_per_car_for_moving_cars=-2.0,\n",
    "        )\n",
    "        \n",
    "    def exp_reward(self, action, exp_rentals, next_morning_counts):\n",
    "        # Modified reward function as described in the question text\n",
    "        if action > 0:\n",
    "            action_reward = self.reward_per_car_for_moving_cars * (action - 1)\n",
    "        else:\n",
    "            action_reward = - self.reward_per_car_for_moving_cars * action\n",
    "        storage_reward = -4.0 * ((next_morning_counts[0] > 10) + (next_morning_counts[1] > 10))\n",
    "        rental_reward = exp_rentals * self.reward_for_rental\n",
    "        return action_reward + storage_reward + rental_reward\n",
    "    \n",
    "# Unit tests to check whether reward is calculated correctly\n",
    "mod_jcr = ModifiedJCR()\n",
    "assert_almost_equal(mod_jcr.exp_reward(3, 5, (7, 13)), -2.0 * (3 - 1)  - 4.0 + 5 * 10.0)\n",
    "assert_almost_equal(mod_jcr.exp_reward(-2, 5, (15, 2)), -2.0 * 2 - 4.0 + 5 * 10.0)\n",
    "assert_almost_equal(mod_jcr.exp_reward(1, 5, (15, 16)), -8.0 + 5 * 10.0)\n",
    "\n",
    "# Solve using policy iteration\n",
    "v = StateValueFunction(mod_jcr)\n",
    "pi = {s: mod_jcr.actions(s)[0] for s in mod_jcr.states}  # arbitrary initial policy\n",
    "policy_iteration(v, pi, gamma=0.9, tol=0.1)\n",
    "\n",
    "# Convert optimal policy to a matrix (dataframe) and visualise\n",
    "pi_df = pd.Series(\n",
    "    pi.values(),\n",
    "    index=pd.MultiIndex.from_tuples(pi.keys(), names=[\"loc1\", \"loc2\"]),\n",
    "    name=\"action\",\n",
    ").unstack()\n",
    "sns.heatmap(pi_df, cmap=\"icefire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ee7f3-fb04-454e-a240-5895b441e397",
   "metadata": {},
   "source": [
    "Some observations (sense checks) on the optimal policy for this modified version of the problem:\n",
    "* Because it is now free to move one car from location 1 to location 2, the optimal policy for the modified problem often does so (considering that location 2 has a mismatch between supply and demand)\n",
    "* There are cliff-edge effects where either location 1 or two crosses threshold of having 10 cars. This make sense due to the new charge to use a second parking lot for any additional cars at each location: where the number of cars exceeds 10 slightly in one location but not the other, it may make sense to move cars to avoid this charge. Note that when locations have well in excess of 10 cars, it no longer makes sense to worry about this, because the cost of moving cars will exceed the (fixed) charge for using a second parking lot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
